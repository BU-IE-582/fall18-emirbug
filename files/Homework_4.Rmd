---
title: "Homework 4"
author: "Emirhan Bugday"
date: "December 19, 2018"
output:
  html_document:
    highlight: tango
    theme: simplex
    toc: yes
    toc_float:
      collapsed: yes
---

```{r, echo = F, warning = F, message=F}
library(data.table)
library(caret)


# Set working directory and folder paths
setwd("G:\\My Drive\\Z.ME\\Boun\\IE 582\\HW-4")


```

## DataSets {.tabset .tabset-fade}

Following datasets are acquired from the UCI machine learning repository (http://archive.ics.uci.edu/ml/datasets.html).

### Student Performance

**Data Set Characteristics:** Multivariate

**Number of Instances:** 649

**Area:** Social

**Attribute Characteristics:** Numeric, categorical, ordinal

**Number of Attributes:** 33

**Date Donated:** 2014-11-27

**Associated Tasks:** Classification, Regression

**Missing Values:** N/A

-----------------------

**Data Set Information:**

This data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). In [Cortez and Silva, 2008], the two datasets were modeled under binary/five-level classification and regression tasks. Important note: the target attribute G3 has a strong correlation with attributes G2 and G1. This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details).

**Attribute Information:**

1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira) 

2 sex - student's sex (binary: 'F' - female or 'M' - male) 

3 age - student's age (numeric: from 15 to 22) 

4 address - student's home address type (binary: 'U' - urban or 'R' - rural) 

5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3) 

6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart) 

7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â???" 5th to 9th grade, 3 â???" secondary 
education or 4 â???" higher education) 

8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â???" 5th to 9th grade, 3 â???" secondary 
education or 4 â???" higher education) 

9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') 

10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') 

11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other') 

12 guardian - student's guardian (nominal: 'mother', 'father' or 'other') 

13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour) 

14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours) 

15 failures - number of past class failures (numeric: n if 1<=n<3, else 4) 

16 schoolsup - extra educational support (binary: yes or no) 

17 famsup - family educational support (binary: yes or no) 

18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no) 

19 activities - extra-curricular activities (binary: yes or no) 

20 nursery - attended nursery school (binary: yes or no) 

21 higher - wants to take higher education (binary: yes or no) 

22 internet - Internet access at home (binary: yes or no) 

23 romantic - with a romantic relationship (binary: yes or no) 

24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) 

25 freetime - free time after school (numeric: from 1 - very low to 5 - very high) 

26 goout - going out with friends (numeric: from 1 - very low to 5 - very high) 

27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) 

28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) 

29 health - current health status (numeric: from 1 - very bad to 5 - very good) 

30 absences - number of school absences (numeric: from 0 to 93) 

31 G1 - first period grade (numeric: from 0 to 20) 

31 G2 - second period grade (numeric: from 0 to 20) 

32 G3 - final grade (numeric: from 0 to 20, output target)



### Breast Cancer Wisconsin

**Data Set Characteristics:** Multivariate

**Number of Instances:** 569

**Area:** Life

**Attribute Characteristics:** Real

**Number of Attributes:** 32

**Date Donated:** 1995-11-01

**Associated Tasks:** Classification

**Missing Values:** No

-------------------------

**Data Set Information:**

  Each record represents follow-up data for one breast cancer
	case.  These are consecutive patients seen by Dr. Wolberg
	since 1984, and include only those cases exhibiting invasive
	breast cancer and no evidence of distant metastases at the
	time of diagnosis. 

	The first 30 features are computed from a digitized image of a
	fine needle aspirate (FNA) of a breast mass.  They describe
	characteristics of the cell nuclei present in the image.
	A few of the images can be found at (http://www.cs.wisc.edu/~street/images/)

**Attribute Information:**

1) ID number 

2) Diagnosis (M = malignant, B = benign) 

3) -32 

Ten real-valued features are computed for each cell nucleus: 

a) radius (mean of distances from center to points on the perimeter) 

b) texture (standard deviation of gray-scale values) 

c) perimeter 

d) area 

e) smoothness (local variation in radius lengths) 

f) compactness (perimeter^2 / area - 1.0) 

g) concavity (severity of concave portions of the contour) 

h) concave points (number of concave portions of the contour) 

i) symmetry 

j) fractal dimension ("coastline approximation" - 1)


### Sports Articles for Objectivity Analysis

**Data Set Characteristics:** Multivariate, Text

**Number of Instances:** 1000

**Area:** Social

**Attribute Characteristics:** integer

**Number of Attributes:** 59

**Date Donated:** 2018-04-09

**Associated Tasks:** Classification

**Missing Values:** No

--------------------------------------

**Data Set Information:**

Some of the features are retrieved using the Stanford POS tagger and the tags are as defined in Penn Treebank Project.

**Attribute Information:**

TextID	text file name 

URL	link to article 

Label	objective vs. subjective 

totalWordsCount	total number of words in the article 

semanticobjscore	Frequency of words with an objective SENTIWORDNET score 

semanticsubjscore	Frequency of words with a subjective SENTIWORDNET score 

CC	Frequency of coordinating conjunctions 

CD	Frequency of numerals and cardinals 

DT	Frequency of determiners 

EX	Frequency of existential there 

FW	Frequency of foreign words 

INs	Frequency of subordinating preposition or conjunction 

JJ	Frequency of ordinal adjectives or numerals 

JJR	Frequency of comparative adjectives 

JJS	Frequency of superlative adjectives 

LS	Frequency of list item markers 

MD	Frequency of modal auxiliaries 

NN	Frequency of singular common nouns 

NNP	Frequency of singular proper nouns 

NNPS	Frequency of plural proper nouns 

NNS	Frequency of plural common nouns 

PDT	Frequency of pre-determiners 

POS	Frequency of genitive markers 

PRP	Frequency of personal pronouns 

PRP$	Frequency of possessive pronouns 

RB	Frequency of adverbs 

RBR	Frequency of comparative adverbs 

RBS	Frequency of superlative adverbs 

RP	Frequency of particles 

SYM	Frequency of symbols 

TOs	Frequency of 'to' as preposition or infinitive marker 

UH	Frequency of interjections 

VB	Frequency of base form verbs 

VBD	Frequency of past tense verbs 

VBG	Frequency of present participle or gerund verbs 

VBN	Frequency of past participle verbs 

VBP	Frequency of present tense verbs with plural 3rd person subjects 

VBZ	Frequency of present tense verbs with singular 3rd person subjects 

WDT	Frequency of WH-determiners 

WP	Frequency of WH-pronouns 

WP$	Frequency of possessive WH-pronouns 

WRB	Frequency of WH-adverbs 

baseform	Frequency of infinitive verbs (base form verbs preceded by â???otoâ???) 

Quotes	Frequency of quotation pairs in the entire article 

questionmarks	Frequency of questions marks in the entire article 

exclamationmarks	Frequency of exclamation marks in the entire article 

fullstops	Frequency of full stops 

commas	Frequency of commas 

semicolon	Frequency of semicolons 

colon	Frequency of colons 

ellipsis	Frequency of ellipsis 

pronouns1st	Frequency of first person pronouns (personal and possessive) 

pronouns2nd	Frequency of second person pronouns (personal and possessive) 

pronouns3rd	Frequency of third person pronouns (personal and possessive) 

compsupadjadv	Frequency of comparative and superlative adjectives and adverbs 

past	Frequency of past tense verbs with 1st and 2nd person pronouns 

imperative	Frequency of imperative verbs 

present3rd	Frequency of present tense verbs with 3rd person pronouns 

present1st2nd	Frequency of present tense verbs with 1st and 2nd person pronouns 

sentence1st	First sentence class 

sentencelast	Last sentence class 

txtcomplexity	Text complexity score


### Gesture Phase Segmentation

**Data Set Characteristics:** Multivariate, Text

**Number of Instances:** 9900

**Area:** N/A

**Attribute Characteristics:** integer

**Number of Attributes:** 50

**Date Donated:** 2014-06-18

**Associated Tasks:** Classification, Clustering

**Missing Values:** No

------------------------------


**Data Set Information:**

The dataset is composed by features extracted from 7 videos with people gesticulating, aiming at studying Gesture Phase Segmentation.

Each video is represented by two files: a raw file, which contains the position of hands, wrists, head and spine of the user in each frame; and a processed file, which contains velocity and acceleration of hands and wrists. See the data set description for more information on the dataset.


**Attribute Information:**

Raw files: 18 numeric attributes (double), a timestamp and a class attribute (nominal). 

Processed files: 32 numeric attributes (double) and a class attribute (nominal). 

A feature vector with up to 50 numeric attributes can be generated with the two files mentioned above.


## Methodology {.tabset .tabset-fade}

1 - Data is fed in the suitable format

2 - 3 folds of test/train splits are created:

  * min of 200 obs or .20 percent of the training data is splitted in each fold
  
  * Stratification w.r.t. Dependent variable is taken into consideration
  
3 - Parameter Tunning for methods are performed  with 5 Fold CV 6 and 2 Resampling:

  * 5 Fold-CV is performed for each fold stated in step 2.
  
  * random seed is set for each fold acros the methods to ensure the resamplings will be the same for each model
  
  * Tuned parameters are stated below



### PRA 

* lasso penalty is used

* lambda: 0.01 0.06 0.11 0.16 0.21 0.26

### DT 

* complexity parameter: 0.01 0.06 0.11 0.16 0.21 0.26

### SVM 

1) kernel type -> Polynomial:

* degree

* scale

2) kernel type -> Radial:

* sigma

### RF 

* m: 2, 5, 10, 25, 50

### SGB 

* depth: 1, 5, 9

* n.trees: 50, 100, 150

* learning rate: 0.05, 0.1, 0.2



## Results

### Student Performance {.tabset .tabset-fade}

* Regression Problem

**Overall Test Performance of the Methods (After Parameter Tuning)**


```{r echo = F}

# Plot the Result
load("Data1.RData")

testPerformances[, .(CV_RMSE = mean(RMSE), CV_Rsquared = mean(Rsquared), CV_MAE = mean(MAE)), ModelName]

testPerformances[order(ModelName, Fold), -3]


```




#### Fold 1

Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold1,
                          DT = modelObjects$DT$Fold1,
                          RF = modelObjects$RF$Fold1,
                          GBM = modelObjects$GBM$Fold1))


bwplot(resamps, layout = c(3, 1))


modelObjects$PRA$Fold1
plot(modelObjects$PRA$Fold1)

modelObjects$DT$Fold1
plot(modelObjects$DT$Fold1)

modelObjects$RF$Fold1
plot(modelObjects$RF$Fold1)

modelObjects$GBM$Fold1
plot(modelObjects$GBM$Fold1)

```


#### Fold 2
Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold2,
                          DT = modelObjects$DT$Fold2,
                          RF = modelObjects$RF$Fold2,
                          GBM = modelObjects$GBM$Fold2))


bwplot(resamps, layout = c(3, 1))


modelObjects$PRA$Fold2
plot(modelObjects$PRA$Fold2)

modelObjects$DT$Fold2
plot(modelObjects$DT$Fold2)

modelObjects$RF$Fold2
plot(modelObjects$RF$Fold2)

modelObjects$GBM$Fold1
plot(modelObjects$GBM$Fold2)

```

#### Fold 3
Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold3,
                          DT = modelObjects$DT$Fold3,
                          RF = modelObjects$RF$Fold3,
                          GBM = modelObjects$GBM$Fold3))


bwplot(resamps, layout = c(3, 1))


modelObjects$PRA$Fold3
plot(modelObjects$PRA$Fold3)

modelObjects$DT$Fold3
plot(modelObjects$DT$Fold3)

modelObjects$RF$Fold3
plot(modelObjects$RF$Fold3)

modelObjects$GBM$Fold3
plot(modelObjects$GBM$Fold3)

```

### Breast Cancer Wisconsin {.tabset .tabset-fade}

* Classification Problem

**Overall Test Performance of the Methods (After Parameter Tuning)**


```{r echo = F}

# Plot the Result
load("Data2.RData")

testPerformances[, .(CV_Accuracy = mean(Accuracy), CV_Kappa = mean(Kappa)), ModelName]

testPerformances[order(ModelName, Fold), -3]


```

**Class Distributions:**

```{r echo = F}
table(data$DependentVariable)
```



#### Fold 1

Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold1,
                          DT = modelObjects$DT$Fold1,
                          RF = modelObjects$RF$Fold1,
                          GBM = modelObjects$GBM$Fold1))


bwplot(resamps, layout = c(2, 1))


modelObjects$PRA$Fold1
plot(modelObjects$PRA$Fold1)

modelObjects$DT$Fold1
plot(modelObjects$DT$Fold1)

modelObjects$RF$Fold1
plot(modelObjects$RF$Fold1)

modelObjects$GBM$Fold1
plot(modelObjects$GBM$Fold1)

```


#### Fold 2
Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold2,
                          DT = modelObjects$DT$Fold2,
                          RF = modelObjects$RF$Fold2,
                          GBM = modelObjects$GBM$Fold2))


bwplot(resamps, layout = c(3, 1))


modelObjects$PRA$Fold2
plot(modelObjects$PRA$Fold2)

modelObjects$DT$Fold2
plot(modelObjects$DT$Fold2)

modelObjects$RF$Fold2
plot(modelObjects$RF$Fold2)

modelObjects$GBM$Fold1
plot(modelObjects$GBM$Fold2)

```

#### Fold 3
Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold3,
                          DT = modelObjects$DT$Fold3,
                          RF = modelObjects$RF$Fold3,
                          GBM = modelObjects$GBM$Fold3))


bwplot(resamps, layout = c(3, 1))


modelObjects$PRA$Fold3
plot(modelObjects$PRA$Fold3)

modelObjects$DT$Fold3
plot(modelObjects$DT$Fold3)

modelObjects$RF$Fold3
plot(modelObjects$RF$Fold3)

modelObjects$GBM$Fold3
plot(modelObjects$GBM$Fold3)

```

### Sports Articles for Objectivity Analysis {.tabset .tabset-fade}

* Classification Problem

**Overall Test Performance of the Methods (After Parameter Tuning)**


```{r echo = F}

# Plot the Result
load("Data3.RData")

testPerformances[, .(CV_Accuracy = mean(Accuracy), CV_Kappa = mean(Kappa)), ModelName]

testPerformances[order(ModelName, Fold), -3]


```

**Class Distributions:**

```{r echo = F}
table(data$DependentVariable)
```



#### Fold 1

Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold1,
                          # DT = modelObjects$DT$Fold1,
                          RF = modelObjects$RF$Fold1,
                          GBM = modelObjects$GBM$Fold1))


bwplot(resamps, layout = c(2, 1))


modelObjects$PRA$Fold1
plot(modelObjects$PRA$Fold1)

# modelObjects$DT$Fold1
# plot(modelObjects$DT$Fold1)

modelObjects$RF$Fold1
plot(modelObjects$RF$Fold1)

modelObjects$GBM$Fold1
plot(modelObjects$GBM$Fold1)

```


#### Fold 2
Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold2,
                          # DT = modelObjects$DT$Fold2,
                          RF = modelObjects$RF$Fold2,
                          GBM = modelObjects$GBM$Fold2))


bwplot(resamps, layout = c(3, 1))


modelObjects$PRA$Fold2
plot(modelObjects$PRA$Fold2)

# modelObjects$DT$Fold2
# plot(modelObjects$DT$Fold2)

modelObjects$RF$Fold2
plot(modelObjects$RF$Fold2)

modelObjects$GBM$Fold1
plot(modelObjects$GBM$Fold2)

```

#### Fold 3
Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold3,
                          # DT = modelObjects$DT$Fold3,
                          RF = modelObjects$RF$Fold3,
                          GBM = modelObjects$GBM$Fold3))


bwplot(resamps, layout = c(3, 1))


modelObjects$PRA$Fold3
plot(modelObjects$PRA$Fold3)

# modelObjects$DT$Fold3
# plot(modelObjects$DT$Fold3)

modelObjects$RF$Fold3
plot(modelObjects$RF$Fold3)

modelObjects$GBM$Fold3
plot(modelObjects$GBM$Fold3)

```

### Gesture Phase Segmentation {.tabset .tabset-fade}

* Classification Problem

**Overall Test Performance of the Methods (After Parameter Tuning)**


```{r echo = F}

# Plot the Result
load("Data4.RData")

testPerformances[, .(CV_Accuracy = mean(Accuracy), CV_Kappa = mean(Kappa)), ModelName]

testPerformances[order(ModelName, Fold), -3]


```

**Class Distributions:**

```{r echo = F}
table(data$DependentVariable)
```


#### Fold 1

Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold1,
                          DT = modelObjects$DT$Fold1,
                          RF = modelObjects$RF$Fold1,
                          GBM = modelObjects$GBM$Fold1))


bwplot(resamps, layout = c(2, 1))


modelObjects$PRA$Fold1
plot(modelObjects$PRA$Fold1)

modelObjects$DT$Fold1
plot(modelObjects$DT$Fold1)

modelObjects$RF$Fold1
plot(modelObjects$RF$Fold1)

modelObjects$GBM$Fold1
plot(modelObjects$GBM$Fold1)

```


#### Fold 2
Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold2,
                          DT = modelObjects$DT$Fold2,
                          RF = modelObjects$RF$Fold2,
                          GBM = modelObjects$GBM$Fold2))


bwplot(resamps, layout = c(3, 1))


modelObjects$PRA$Fold2
plot(modelObjects$PRA$Fold2)

modelObjects$DT$Fold2
plot(modelObjects$DT$Fold2)

modelObjects$RF$Fold2
plot(modelObjects$RF$Fold2)

modelObjects$GBM$Fold1
plot(modelObjects$GBM$Fold2)

```

#### Fold 3
Parameter tuning results for the methods are provided below

```{r echo = F}
resamps <- resamples(list(PRA = modelObjects$PRA$Fold3,
                          DT = modelObjects$DT$Fold3,
                          RF = modelObjects$RF$Fold3,
                          GBM = modelObjects$GBM$Fold3))


bwplot(resamps, layout = c(3, 1))


modelObjects$PRA$Fold3
plot(modelObjects$PRA$Fold3)

modelObjects$DT$Fold3
plot(modelObjects$DT$Fold3)

modelObjects$RF$Fold3
plot(modelObjects$RF$Fold3)

modelObjects$GBM$Fold3
plot(modelObjects$GBM$Fold3)

```

## Summary

a) In most of the cases cross-validation error rates are consistent with the test error rates. However we can observe fluctuation in cross-validation error rates, actually that is the reason we apply cross-validation - to obtain a more robust error rate for a method - while comparing the performances of different models as well as determining hyper parameter. (Individual error rates and the confidence interval for each case w.r.t folds & resamplings are provided above)

b) For the used data sets and the provided settings, we can conclude that Stochastic Gradient Boosting performs better than the other methods for classification problems. On the other hand, Random Forest provides the best result on the regression problem.

c) test errors are higher observed to be higher than the training errors. For the most of the cases the difference is not big so we can conclude that the methods has a stable performances on the data sets.


Source codes are provided in the appendix

## Appendix

### Student Performance

```{r echo = T, eval = F}
rm(list = ls())
setwd("G:/My Drive/Z.ME/Boun/IE 582/HW-4/DataSets")

library(data.table)
library(caret)

# Load Data
data <- fread(file = "student/student-por.csv")

# Run Parameters
IsRegression <-1

# Prepare Data
data <- data[,-c("G1", "G2"), with = FALSE]
str(data)
dim(data)

# Rename Dependent Variable
names(data)[names(data) == "G3"] <- "DependentVariable"
table(data$DependentVariable)


if (IsRegression == 0) {
  
  data$DependentVariable <- as.factor(data$DependentVariable)
  
}

# Create Folds for robust test performance
minTestObsFreq <- max(200/dim(data)[1], 0.2)
nFold <- 3

# Take stratification into consideration while splitting
folds <- createDataPartition(data$DependentVariable, p = minTestObsFreq, list = FALSE, times = nFold)
head(folds)


# Experiment
modelObjects <- list()
testPerformances <- data.table()


for (f in c(1:nFold)) {
  
  # Set Random Seed to ensure the resamplings will be the same for each model
  set.seed(7 * f)
  
  # Create Test/Train Data sets
  training <- data[folds[,f],]
  testing <- data[-folds[,f],]
  
  # Define Train CV and Resampling parametes
  fitControl <- trainControl( method = "repeatedcv",
                              ## 5-fold CV
                              number = 5,
                              ## repeated 2 times
                              repeats = 2)
  
  try({
  #### PRA #####
  print("PRA")
  time <- Sys.time()
  
  # Parameter selection for PRA model 
  praGrid <-  expand.grid(alpha = 1, # To use lasso penalty
                          lambda = seq(0.01,0.3,0.05))
  
  
  # PRA Train and Parameter Tuning
  praFit <- train(DependentVariable ~ ., data = training, 
                  method = "glmnet", 
                  trControl = fitControl,
                  tuneGrid = praGrid)
  
  modelObjects[["PRA"]][[paste0("Fold", f)]] <- praFit
  
  praFit
  
  plot(praFit)
  
  # Predictions for the fold
  Prediction <- predict.train(praFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "PRA", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  })
  
  #### DT #####
  print("DT")
  time <- Sys.time()
  
  # Parameter selection for DT model 
  dtGrid <-  expand.grid(cp = seq(0.01,0.3,0.05))
  
  
  # DT Train and Parameter Tuning
  dtFit <- train(DependentVariable ~ ., data = training, 
                 method = "rpart", 
                 trControl = fitControl,
                 tuneGrid = dtGrid)
  
  modelObjects[["DT"]][[paste0("Fold", f)]] <- dtFit
  
  dtFit
  
  plot(dtFit)
  
  # Predictions for the fold
  Prediction <- predict.train(dtFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "DT", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  
  
  #### SVM #####
  print("SVM")
  time <- Sys.time()
  
  if (IsRegression == 5) {
    
    # Parameter selection for SVM model 
    # svmPolyGrid <-  expand.grid()
    
    
    # SVM Train and Parameter Tuning
    svmPolyFit <- train(DependentVariable ~ ., data = training, 
                        method = "lssvmPoly", 
                        trControl = fitControl, 
                        # preProc = c("center", "scale"),
                        tuneLength = 8)
    
    modelObjects[["SVMPoly"]][[paste0("Fold", f)]] <- svmPolyFit
    
    svmPolyFit
    
    plot(svmPolyFit)
    
    # Predictions for the fold
    Prediction <- predict.train(svmPolyFit, newdata = testing)
    
    # Test Metrics
    testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
    
    # Save the test metrics
    testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "SVMPoly", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
    
    
    
    # Parameter selection for SVM model 
    # svmRadialGrid <-  expand.grid()
    
    
    # SVM Train and Parameter Tuning
    svmRadialFit <- train(DependentVariable ~ ., data = training, 
                        method = "lssvmRadial", 
                        trControl = fitControl, 
                        # preProc = c("center", "scale"),
                        tuneLength = 8)
    
    modelObjects[["SVMRadial"]][[paste0("Fold", f)]] <- svmRadialFit
    
    svmRadialFit
    
    plot(svmRadialFit)
    
    # Predictions for the fold
    Prediction <- predict.train(svmRadialFit, newdata = testing)
    
    # Test Metrics
    testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
    
    # Save the test metrics
    testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "SVMRadial", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
    
  
  }
  
  
  
  #### RF #####
  print("RF")
  time <- Sys.time()
  
  # Parameter selection for GBM model 
  rfGrid <-  expand.grid(mtry = c(2, 5, 10, 25, 50))
  
  # GBM Train and Parameter Tuning
  rfFit <- train( DependentVariable ~ ., data = training, 
                   method = "rf", 
                   trControl = fitControl,
                   tuneGrid = rfGrid,
                   verbose = FALSE)
  
  
  modelObjects[["RF"]][[paste0("Fold", f)]] <- rfFit
  
  rfFit
  
  plot(rfFit)
  
  # Predictions for the fold
  Prediction <- predict.train(rfFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "RF", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  
  
  
  #### SGB #####
  print("SGB")
  time <- Sys.time()
  
  # Parameter selection for GBM model 
  gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                          n.trees = c(50, 100, 150), 
                          shrinkage = c(0.05, 0.1, 0.2),
                          n.minobsinnode = 5)
  
  # GBM Train and Parameter Tuning
  gbmFit <- train( DependentVariable ~ ., data = training, 
                   method = "gbm", 
                   trControl = fitControl,
                   tuneGrid = gbmGrid,
                   verbose = FALSE)
  
  
  modelObjects[["GBM"]][[paste0("Fold", f)]] <- gbmFit
  
  gbmFit
  
  plot(gbmFit)
  
  # Predictions for the fold
  Prediction <- predict.train(gbmFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "GBM", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
}

save.image(file = "G:\\My Drive\\Z.ME\\Boun\\IE 582\\HW-4\\Data1.RData")

```

### Breast Cancer Wisconsin

```{r echo = T, eval = F}
rm(list = ls())
setwd("G:/My Drive/Z.ME/Boun/IE 582/HW-4/DataSets")

library(data.table)
library(caret)

# Load Data
data <- fread(file = "wdbc.data")

# Run Parameters
IsRegression <- 0

# Prepare Data
data <- data[,-c("V1"), with = FALSE]
str(data)
dim(data)

# Rename Dependent Variable
names(data)[names(data) == "V2"] <- "DependentVariable"
table(data$DependentVariable)


if (IsRegression == 0) {
  
  data$DependentVariable <- as.factor(data$DependentVariable)
  
}

# Create Folds for robust test performance
minTestObsFreq <- max(200/dim(data)[1], 0.2)
nFold <- 3

# Take stratification into consideration while splitting
folds <- createDataPartition(data$DependentVariable, p = minTestObsFreq, list = FALSE, times = nFold)
head(folds)


# Experiment
modelObjects <- list()
testPerformances <- data.table()


for (f in c(1:nFold)) {
  
  # Set Random Seed to ensure the resamplings will be the same for each model
  set.seed(7 * f)
  
  # Create Test/Train Data sets
  training <- data[folds[,f],]
  testing <- data[-folds[,f],]
  
  # Define Train CV and Resampling parametes
  fitControl <- trainControl( method = "repeatedcv",
                              ## 5-fold CV
                              number = 5,
                              ## repeated 2 times
                              repeats = 2)
  
  try({
  #### PRA #####
  print("PRA")
  time <- Sys.time()
  
  # Parameter selection for PRA model 
  praGrid <-  expand.grid(alpha = 1, # To use lasso penalty
                          lambda = seq(0.01,0.3,0.05))
  
  
  # PRA Train and Parameter Tuning
  praFit <- train(DependentVariable ~ ., data = training, 
                  method = "glmnet", 
                  trControl = fitControl,
                  tuneGrid = praGrid)
  
  modelObjects[["PRA"]][[paste0("Fold", f)]] <- praFit
  
  praFit
  
  plot(praFit)
  
  # Predictions for the fold
  Prediction <- predict.train(praFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "PRA", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  })
  
  #### DT #####
  print("DT")
  time <- Sys.time()
  
  # Parameter selection for DT model 
  dtGrid <-  expand.grid(cp = seq(0.01,0.3,0.05))
  
  
  # DT Train and Parameter Tuning
  dtFit <- train(DependentVariable ~ ., data = training, 
                 method = "rpart", 
                 trControl = fitControl,
                 tuneGrid = dtGrid)
  
  modelObjects[["DT"]][[paste0("Fold", f)]] <- dtFit
  
  dtFit
  
  plot(dtFit)
  
  # Predictions for the fold
  Prediction <- predict.train(dtFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "DT", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  
  
  #### SVM #####
  print("SVM")
  time <- Sys.time()
  
  if (IsRegression == 5) {
    
    # Parameter selection for SVM model 
    # svmPolyGrid <-  expand.grid()
    
    
    # SVM Train and Parameter Tuning
    svmPolyFit <- train(DependentVariable ~ ., data = training, 
                        method = "lssvmPoly", 
                        trControl = fitControl, 
                        # preProc = c("center", "scale"),
                        tuneLength = 8)
    
    modelObjects[["SVMPoly"]][[paste0("Fold", f)]] <- svmPolyFit
    
    svmPolyFit
    
    plot(svmPolyFit)
    
    # Predictions for the fold
    Prediction <- predict.train(svmPolyFit, newdata = testing)
    
    # Test Metrics
    testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
    
    # Save the test metrics
    testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "SVMPoly", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
    
    
    
    # Parameter selection for SVM model 
    # svmRadialGrid <-  expand.grid()
    
    
    # SVM Train and Parameter Tuning
    svmRadialFit <- train(DependentVariable ~ ., data = training, 
                        method = "lssvmRadial", 
                        trControl = fitControl, 
                        # preProc = c("center", "scale"),
                        tuneLength = 8)
    
    modelObjects[["SVMRadial"]][[paste0("Fold", f)]] <- svmRadialFit
    
    svmRadialFit
    
    plot(svmRadialFit)
    
    # Predictions for the fold
    Prediction <- predict.train(svmRadialFit, newdata = testing)
    
    # Test Metrics
    testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
    
    # Save the test metrics
    testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "SVMRadial", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
    
  
  }
  
  
  
  #### RF #####
  print("RF")
  time <- Sys.time()
  
  # Parameter selection for GBM model 
  rfGrid <-  expand.grid(mtry = c(2, 5, 10, 25, 50))
  
  # GBM Train and Parameter Tuning
  rfFit <- train( DependentVariable ~ ., data = training, 
                   method = "rf", 
                   trControl = fitControl,
                   tuneGrid = rfGrid,
                   verbose = FALSE)
  
  
  modelObjects[["RF"]][[paste0("Fold", f)]] <- rfFit
  
  rfFit
  
  plot(rfFit)
  
  # Predictions for the fold
  Prediction <- predict.train(rfFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "RF", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  
  
  
  #### SGB #####
  print("SGB")
  time <- Sys.time()
  
  # Parameter selection for GBM model 
  gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                          n.trees = c(50, 100, 150), 
                          shrinkage = c(0.05, 0.1, 0.2),
                          n.minobsinnode = 5)
  
  # GBM Train and Parameter Tuning
  gbmFit <- train( DependentVariable ~ ., data = training, 
                   method = "gbm", 
                   trControl = fitControl,
                   tuneGrid = gbmGrid,
                   verbose = FALSE)
  
  
  modelObjects[["GBM"]][[paste0("Fold", f)]] <- gbmFit
  
  gbmFit
  
  plot(gbmFit)
  
  # Predictions for the fold
  Prediction <- predict.train(gbmFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "GBM", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
}

save.image(file = "G:\\My Drive\\Z.ME\\Boun\\IE 582\\HW-4\\Data2.RData")

```

### Sports Articles for Objectivity Analysis

```{r echo = T, eval = F}
rm(list = ls())
setwd("G:/My Drive/Z.ME/Boun/IE 582/HW-4/DataSets")

library(data.table)
library(caret)

# Load Data
data <- fread(file = "SportsArticles/features.csv", header = T) 

# Run Parameters
IsRegression <- 0

# Prepare Data
data <- data[, -c("TextID", "URL"), with = F]
str(data)
dim(data)


# Rename Dependent Variable
names(data)[names(data) == "Label"] <- "DependentVariable"
table(data$DependentVariable)

if (IsRegression == 0) {
  
  data$DependentVariable <- as.factor(data$DependentVariable)
  
}

# Create Folds for robust test performance
minTestObsFreq <- max(200/dim(data)[1], 0.2)
nFold <- 3

# Take stratification into consideration while splitting
folds <- createDataPartition(data$DependentVariable, p = minTestObsFreq, list = FALSE, times = nFold)
head(folds)


# Experiment
modelObjects <- list()
testPerformances <- data.table()


for (f in c(1:nFold)) {
  
  # Set Random Seed to ensure the resamplings will be the same for each model
  set.seed(7 * f)
  
  # Create Test/Train Data sets
  training <- data[folds[,f],]
  testing <- data[-folds[,f],]
  
  # Define Train CV and Resampling parametes
  fitControl <- trainControl( method = "repeatedcv",
                              ## 5-fold CV
                              number = 5,
                              ## repeated 2 times
                              repeats = 2)
  
  try({
  #### PRA #####
  print("PRA")
  time <- Sys.time()
  
  # Parameter selection for PRA model 
  praGrid <-  expand.grid(alpha = 1, # To use lasso penalty
                          lambda = seq(0.01,0.3,0.05))
  
  
  # PRA Train and Parameter Tuning
  praFit <- train(DependentVariable ~ ., data = training, 
                  method = "glmnet", 
                  trControl = fitControl,
                  tuneGrid = praGrid)
  
  modelObjects[["PRA"]][[paste0("Fold", f)]] <- praFit
  
  praFit
  
  plot(praFit)
  
  # Predictions for the fold
  Prediction <- predict.train(praFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "PRA", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  })
  
  try({
  #### DT #####
  print("DT")
  time <- Sys.time()
  
  # Parameter selection for DT model 
  dtGrid <-  expand.grid(cp = seq(0.01,0.3,0.05))
  
  
  # DT Train and Parameter Tuning
  dtFit <- train(DependentVariable ~ ., data = training, 
                 method = "rpart", 
                 trControl = fitControl,
                 tuneGrid = dtGrid)
  
  modelObjects[["DT"]][[paste0("Fold", f)]] <- dtFit
  
  dtFit
  
  plot(dtFit)
  
  # Predictions for the fold
  Prediction <- predict.train(dtFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "DT", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  })
  
  
  #### SVM #####
  print("SVM")
  time <- Sys.time()
  
  if (IsRegression == 5) {
    
    # Parameter selection for SVM model 
    # svmPolyGrid <-  expand.grid()
    
    
    # SVM Train and Parameter Tuning
    svmPolyFit <- train(DependentVariable ~ ., data = training, 
                        method = "lssvmPoly", 
                        trControl = fitControl, 
                        # preProc = c("center", "scale"),
                        tuneLength = 8)
    
    modelObjects[["SVMPoly"]][[paste0("Fold", f)]] <- svmPolyFit
    
    svmPolyFit
    
    plot(svmPolyFit)
    
    # Predictions for the fold
    Prediction <- predict.train(svmPolyFit, newdata = testing)
    
    # Test Metrics
    testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
    
    # Save the test metrics
    testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "SVMPoly", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
    
    
    
    # Parameter selection for SVM model 
    # svmRadialGrid <-  expand.grid()
    
    
    # SVM Train and Parameter Tuning
    svmRadialFit <- train(DependentVariable ~ ., data = training, 
                        method = "lssvmRadial", 
                        trControl = fitControl, 
                        # preProc = c("center", "scale"),
                        tuneLength = 8)
    
    modelObjects[["SVMRadial"]][[paste0("Fold", f)]] <- svmRadialFit
    
    svmRadialFit
    
    plot(svmRadialFit)
    
    # Predictions for the fold
    Prediction <- predict.train(svmRadialFit, newdata = testing)
    
    # Test Metrics
    testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
    
    # Save the test metrics
    testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "SVMRadial", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
    
  
  }
  
  
  
  #### RF #####
  print("RF")
  time <- Sys.time()
  
  # Parameter selection for GBM model 
  rfGrid <-  expand.grid(mtry = c(2, 5, 10, 25, 50))
  
  # GBM Train and Parameter Tuning
  rfFit <- train( DependentVariable ~ ., data = training, 
                   method = "rf", 
                   trControl = fitControl,
                   tuneGrid = rfGrid,
                   verbose = FALSE)
  
  
  modelObjects[["RF"]][[paste0("Fold", f)]] <- rfFit
  
  rfFit
  
  plot(rfFit)
  
  # Predictions for the fold
  Prediction <- predict.train(rfFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "RF", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  
  
  
  #### SGB #####
  print("SGB")
  time <- Sys.time()
  
  # Parameter selection for GBM model 
  gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                          n.trees = c(50, 100, 150), 
                          shrinkage = c(0.05, 0.1, 0.2),
                          n.minobsinnode = 5)
  
  # GBM Train and Parameter Tuning
  gbmFit <- train( DependentVariable ~ ., data = training, 
                   method = "gbm", 
                   trControl = fitControl,
                   tuneGrid = gbmGrid,
                   verbose = FALSE)
  
  
  modelObjects[["GBM"]][[paste0("Fold", f)]] <- gbmFit
  
  gbmFit
  
  plot(gbmFit)
  
  # Predictions for the fold
  Prediction <- predict.train(gbmFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "GBM", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
}

save.image(file = "G:\\My Drive\\Z.ME\\Boun\\IE 582\\HW-4\\Data3.RData")

```

### Gesture Phase Segmentation

```{r echo = T, eval = F}
rm(list = ls())
setwd("G:/My Drive/Z.ME/Boun/IE 582/HW-4/DataSets")

library(data.table)
library(caret)

# Load Data
data <- fread(file = "gesture_phase_dataset/a1_raw.csv" )

# Run Parameters
IsRegression <- 0

# Prepare Data
data <- data[, -c("timestamp"), with = F]
str(data)
dim(data)

# Rename Dependent Variable
names(data)[names(data) == "phase"] <- "DependentVariable"
table(data$DependentVariable)

if (IsRegression == 0) {
  
  data$DependentVariable <- as.factor(data$DependentVariable)
  
}

# Create Folds for robust test performance
minTestObsFreq <- max(200/dim(data)[1], 0.2)
nFold <- 3

# Take stratification into consideration while splitting
folds <- createDataPartition(data$DependentVariable, p = minTestObsFreq, list = FALSE, times = nFold)
head(folds)


# Experiment
modelObjects <- list()
testPerformances <- data.table()


for (f in c(1:nFold)) {
  
  # Set Random Seed to ensure the resamplings will be the same for each model
  set.seed(7 * f)
  
  # Create Test/Train Data sets
  training <- data[folds[,f],]
  testing <- data[-folds[,f],]
  
  # Define Train CV and Resampling parametes
  fitControl <- trainControl( method = "repeatedcv",
                              ## 5-fold CV
                              number = 5,
                              ## repeated 2 times
                              repeats = 2)
  
  try({
  #### PRA #####
  print("PRA")
  time <- Sys.time()
  
  # Parameter selection for PRA model 
  praGrid <-  expand.grid(alpha = 1, # To use lasso penalty
                          lambda = seq(0.01,0.3,0.05))
  
  
  # PRA Train and Parameter Tuning
  praFit <- train(DependentVariable ~ ., data = training, 
                  method = "glmnet", 
                  trControl = fitControl,
                  tuneGrid = praGrid)
  
  modelObjects[["PRA"]][[paste0("Fold", f)]] <- praFit
  
  praFit
  
  plot(praFit)
  
  # Predictions for the fold
  Prediction <- predict.train(praFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "PRA", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  })
  
  #### DT #####
  print("DT")
  time <- Sys.time()
  
  # Parameter selection for DT model 
  dtGrid <-  expand.grid(cp = seq(0.01,0.3,0.05))
  
  
  # DT Train and Parameter Tuning
  dtFit <- train(DependentVariable ~ ., data = training, 
                 method = "rpart", 
                 trControl = fitControl,
                 tuneGrid = dtGrid)
  
  modelObjects[["DT"]][[paste0("Fold", f)]] <- dtFit
  
  dtFit
  
  plot(dtFit)
  
  # Predictions for the fold
  Prediction <- predict.train(dtFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "DT", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  
  try({
  #### SVM #####
  print("SVM")
  time <- Sys.time()
  
  if (IsRegression == 2) {
    
    # Parameter selection for SVM model 
    # svmPolyGrid <-  expand.grid()
    
    
    # SVM Train and Parameter Tuning
    svmPolyFit <- train(DependentVariable ~ ., data = training, 
                        method = "lssvmPoly", 
                        trControl = fitControl, 
                        # preProc = c("center", "scale"),
                        tuneLength = 8)
    
    modelObjects[["SVMPoly"]][[paste0("Fold", f)]] <- svmPolyFit
    
    svmPolyFit
    
    plot(svmPolyFit)
    
    # Predictions for the fold
    Prediction <- predict.train(svmPolyFit, newdata = testing)
    
    # Test Metrics
    testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
    
    # Save the test metrics
    testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "SVMPoly", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
    
    
    
    # Parameter selection for SVM model 
    # svmRadialGrid <-  expand.grid()
    
    
    # SVM Train and Parameter Tuning
    svmRadialFit <- train(DependentVariable ~ ., data = training, 
                        method = "lssvmRadial", 
                        trControl = fitControl, 
                        # preProc = c("center", "scale"),
                        tuneLength = 8)
    
    modelObjects[["SVMRadial"]][[paste0("Fold", f)]] <- svmRadialFit
    
    svmRadialFit
    
    plot(svmRadialFit)
    
    # Predictions for the fold
    Prediction <- predict.train(svmRadialFit, newdata = testing)
    
    # Test Metrics
    testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
    
    # Save the test metrics
    testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "SVMRadial", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
    
  
  }
  
  })
  
  #### RF #####
  print("RF")
  time <- Sys.time()
  
  # Parameter selection for GBM model 
  rfGrid <-  expand.grid(mtry = c(2, 5, 10, 25, 50))
  
  # GBM Train and Parameter Tuning
  rfFit <- train( DependentVariable ~ ., data = training, 
                   method = "rf", 
                   trControl = fitControl,
                   tuneGrid = rfGrid,
                   verbose = FALSE)
  
  
  modelObjects[["RF"]][[paste0("Fold", f)]] <- rfFit
  
  rfFit
  
  plot(rfFit)
  
  # Predictions for the fold
  Prediction <- predict.train(rfFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "RF", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
  
  
  
  #### SGB #####
  print("SGB")
  time <- Sys.time()
  
  # Parameter selection for GBM model 
  gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                          n.trees = c(50, 100, 150), 
                          shrinkage = c(0.05, 0.1, 0.2),
                          n.minobsinnode = 5)
  
  # GBM Train and Parameter Tuning
  gbmFit <- train( DependentVariable ~ ., data = training, 
                   method = "gbm", 
                   trControl = fitControl,
                   tuneGrid = gbmGrid,
                   verbose = FALSE)
  
  
  modelObjects[["GBM"]][[paste0("Fold", f)]] <- gbmFit
  
  gbmFit
  
  plot(gbmFit)
  
  # Predictions for the fold
  Prediction <- predict.train(gbmFit, newdata = testing)
  
  # Test Metrics
  testResult <- postResample(pred = Prediction, obs = testing$DependentVariable)
  
  # Save the test metrics
  testPerformances <- rbind(testPerformances, cbind(data.table(ModelName = "GBM", Fold = f, Time = (Sys.time() - time)), data.table(t(testResult))))
  
  
}

save.image(file = "G:\\My Drive\\Z.ME\\Boun\\IE 582\\HW-4\\Data4.RData")

```